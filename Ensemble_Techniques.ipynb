{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Techniques"
      ],
      "metadata": {
        "id": "UKeBizopX1Ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "Answer:\n",
        "Ensemble Learning is a machine learning technique where multiple models (called base learners or weak learners) are combined to create a stronger and more accurate model.\n",
        "The key idea is that by aggregating the predictions of several diverse models, the overall system reduces errors caused by bias, variance, or noise.\n",
        "Common ensemble methods include Bagging, Boosting, and Stacking. These approaches aim to improve performance, generalization, and robustness compared to a single model.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "Answer:\n",
        "\n",
        "Feature\tBagging\tBoosting\n",
        "Full Form\tBootstrap Aggregating\t—\n",
        "Goal\tReduce variance\tReduce bias and variance\n",
        "Model Independence\tEach model is trained independently in parallel\tModels are trained sequentially, with each new model correcting previous errors\n",
        "Weighting of Samples\tAll samples are equally weighted\tMisclassified samples get higher weights\n",
        "Examples\tRandom Forest\tAdaBoost, Gradient Boosting, XGBoost\n",
        "Result\tStable and less overfitted model\tHighly accurate but can overfit if not tuned\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "Answer:\n",
        "Bootstrap sampling is a statistical technique where random samples are drawn with replacement from the original dataset to create multiple new training subsets. Each subset is of the same size as the original dataset but may contain duplicate samples.\n",
        "In Bagging methods like Random Forest, bootstrap sampling allows each decision tree to be trained on a slightly different subset of the data. This introduces diversity among the trees, reducing overfitting and improving model stability and accuracy when their predictions are aggregated (e.g., by averaging or voting).\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "Answer:\n",
        "Out-of-Bag (OOB) samples are the data points not selected in a bootstrap sample for training a particular model. Typically, around one-third of the data remains as OOB samples for each base learner.\n",
        "The OOB score is an internal validation metric used in Bagging methods (like Random Forests) to estimate the model’s performance without needing a separate validation set. Each model is evaluated on its corresponding OOB samples, and the combined results give an unbiased estimate of the model’s accuracy or error rate.\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Answer:\n",
        "In a single Decision Tree, feature importance is determined by measuring how much each feature reduces impurity (e.g., Gini impurity or entropy) across all its splits. The more a feature contributes to decreasing impurity, the higher its importance score. However, since the model is based on a single tree, it can be sensitive to noise and overfitting, making its feature importance less reliable.\n",
        "\n",
        "In a Random Forest, feature importance is computed by averaging the importance scores of each feature across all the trees in the ensemble. This aggregation process reduces variance and provides a more stable and robust estimate of feature importance. Random Forest feature importance is generally more accurate and less biased than that of a single Decision Tree because it captures the overall contribution of features across multiple trees trained on different subsets of data.\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "Answer:\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "top_features = feature_importances.sort_values(ascending=False).head(5)\n",
        "\n",
        "# Print top 5 features\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top_features)\n",
        "\n",
        "\n",
        "✅ Example Output:\n",
        "\n",
        "Top 5 Important Features:\n",
        "worst perimeter        0.1758\n",
        "worst concave points   0.1602\n",
        "mean concave points    0.0987\n",
        "worst radius           0.0896\n",
        "mean radius            0.0653\n",
        "dtype: float64\n",
        "\n",
        "\n",
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "Answer:\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "# Compare accuracies\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n",
        "\n",
        "\n",
        "✅ Example Output:\n",
        "\n",
        "Decision Tree Accuracy: 0.9333\n",
        "Bagging Classifier Accuracy: 0.9777\n",
        "\n",
        "Question 8:\n",
        "\n",
        "Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "\n",
        "Answer:\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 7, None]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on test data\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Final Test Accuracy:\", round(accuracy, 4))\n",
        "\n",
        "✅ Example Output:\n",
        "Best Parameters: {'max_depth': 7, 'n_estimators': 100}\n",
        "Final Test Accuracy: 0.9708\n",
        "\n",
        "\n",
        "Explanation:\n",
        "GridSearchCV tests different combinations of parameters using cross-validation to find the optimal configuration.\n",
        "\n",
        "max_depth controls how deep each tree grows (prevents overfitting).\n",
        "\n",
        "n_estimators controls the number of trees in the forest (higher = more robust but slower).\n",
        "The best model is then used to evaluate accuracy on the test set.\n",
        "\n",
        "Q9. Write a Python program to:\n",
        "\n",
        "Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "\n",
        "Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "Answer (code + explanation):\n",
        "\n",
        "# Q9: Train Bagging Regressor and RandomForestRegressor and compare MSE\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train / test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# 1) Bagging Regressor with Decision Tree base estimator\n",
        "base_dt = DecisionTreeRegressor(random_state=42)\n",
        "bag = BaggingRegressor(base_estimator=base_dt, n_estimators=50, random_state=42, n_jobs=-1)\n",
        "bag.fit(X_train, y_train)\n",
        "y_pred_bag = bag.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# 2) Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(f\"Bagging Regressor MSE: {mse_bag:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n",
        "\n",
        "# Optional: percent improvement\n",
        "impr = (mse_bag - mse_rf) / mse_bag * 100\n",
        "print(f\"Random Forest improvement over Bagging: {impr:.2f}%\")\n",
        "\n",
        "\n",
        "Notes & interpretation\n",
        "\n",
        "Both models are ensemble tree-based regressors; Random Forest is a specialized bagging variant that also decorrelates trees by selecting subsets of features at each split.\n",
        "\n",
        "Example output (will vary by split / random_state / hyperparameters):\n",
        "\n",
        "Bagging Regressor MSE: 0.5302\n",
        "Random Forest Regressor MSE: 0.3947\n",
        "Random Forest improvement over Bagging: 25.59%\n",
        "\n",
        "\n",
        "Your exact MSE numbers may differ on your machine or with different random state / hyperparameters. If RF is better, it’s because feature subsampling and averaging reduce variance and improve generalization.\n",
        "\n",
        "Q10. You are a data scientist at a financial institution predicting loan default. Using ensembles — step-by-step approach:\n",
        "\n",
        "Answer:\n",
        "\n",
        "1) Problem framing & data understanding\n",
        "\n",
        "Define the prediction target (binary default: 1 = default, 0 = no default), business costs (cost of false negative vs false positive), and acceptable latency/interpretability requirements.\n",
        "\n",
        "Explore data: feature types (numerical / categorical / ordinal / datetime / text), missingness patterns, class balance, outliers, feature distributions, and data leakage risks.\n",
        "\n",
        "2) Choose between Bagging or Boosting\n",
        "\n",
        "Bagging (e.g., Random Forest): good when base learners (trees) are high-variance and you need robustness and fast training. Simpler to tune, more stable.\n",
        "\n",
        "Boosting (e.g., XGBoost, LightGBM, CatBoost): usually achieves higher accuracy on tabular data, especially with heterogeneous features and complex relationships. It reduces bias by sequentially correcting errors; often the go-to for credit-risk tabular problems.\n",
        "\n",
        "Choice rule: prefer Boosting for best predictive performance on tabular financial data (unless interpretability/latency/regulatory needs favor Random Forest or simpler models).\n",
        "\n",
        "3) Data preprocessing & feature engineering\n",
        "\n",
        "Handle missing values: impute (median/most_frequent), model-based imputation (IterativeImputer) or create missing indicators if missingness is informative.\n",
        "\n",
        "Encode categoricals: target/mean encoding or CatBoost’s native encoding for high-cardinality features; one-hot for low-cardinality.\n",
        "\n",
        "Create domain features: credit utilization, delinquencies per time window, rolling aggregates from transaction history, time-since-last-default, ratios, interaction features.\n",
        "\n",
        "Scale features if you plan to use non-tree models (SVM, logistic); tree ensembles generally don’t require scaling.\n",
        "\n",
        "4) Handle class imbalance\n",
        "\n",
        "Evaluate class balance; if minority (defaults) are rare:\n",
        "\n",
        "Use class weighting (e.g., scale_pos_weight in XGBoost, class_weight='balanced' for sklearn).\n",
        "\n",
        "Use careful resampling: SMOTE or undersampling (but be cautious with time-series / leakage).\n",
        "\n",
        "Optimize metrics suited to imbalance (AUC, PR-AUC, recall at fixed precision, cost-based metrics).\n",
        "\n",
        "5) Select base models\n",
        "\n",
        "Start with tree-based ensembles: XGBoost, LightGBM, CatBoost, RandomForest.\n",
        "\n",
        "For interpretability baseline, also train Logistic Regression (with regularization) — useful for comparison and regulatory explanations.\n",
        "\n",
        "If latency or interpretability is critical, consider simpler ensembles or small trees.\n",
        "\n",
        "6) Prevent & handle overfitting\n",
        "\n",
        "Use cross-validation (prefer time-aware splits if data is temporal).\n",
        "\n",
        "Regularization: learning rate, max_depth, min_child_weight (XGBoost/LightGBM), colsample_bytree, subsample, n_estimators with early stopping.\n",
        "\n",
        "Early stopping on a validation set: stop boosting when validation metric plateaus.\n",
        "\n",
        "Use nested CV for reliable performance estimates when tuning hyperparameters.\n",
        "\n",
        "Feature selection / dimensionality reduction to remove noisy features.\n",
        "\n",
        "Calibration of predicted probabilities (Platt scaling, isotonic) if probabilities are used for decisions.\n",
        "\n",
        "7) Hyperparameter tuning & validation\n",
        "\n",
        "Use Stratified K-Fold for non-temporal data; use time-series split for temporal data.\n",
        "\n",
        "Use RandomizedSearchCV or Bayesian optimization (Optuna) for efficient hyperparameter search (tuning learning rate, max_depth, n_estimators, colsample, subsample, regularization terms).\n",
        "\n",
        "Use metrics aligned to business goals: ROC-AUC, PR-AUC, Recall @ fixed Precision, and expected monetary value (cost matrix).\n",
        "\n",
        "Use nested CV if you need an unbiased generalization estimate after tuning.\n",
        "\n",
        "8) Model evaluation and business-aware metrics\n",
        "\n",
        "Compute confusion matrix, precision, recall, F1, ROC-AUC, PR-AUC. Also compute business KPIs: expected loss reduction, lift, KS statistic, predicted default rates per score band.\n",
        "\n",
        "Perform calibration checks (reliability diagrams) — crucial when probabilities drive decisions (e.g., credit limits).\n",
        "\n",
        "Evaluate model across subgroups (age, region) for fairness and regulatory compliance.\n",
        "\n",
        "9) Interpretability & explainability\n",
        "\n",
        "Use feature importance and SHAP values for local and global explanations; produce human-readable rules for top-risk cases.\n",
        "\n",
        "Provide clear documentation for regulators: features used, data sources, validation results, and expected failure modes.\n",
        "\n",
        "10) Deployment & monitoring\n",
        "\n",
        "Define decision thresholds using business cost trade-offs (cost of missed defaults vs cost of rejecting good customers).\n",
        "\n",
        "Build monitoring: data drift detection, model performance degradation, changes in calibration, and periodic retraining cadence.\n",
        "\n",
        "Log predictions and outcomes for audits.\n",
        "\n",
        "11) Why ensemble learning improves decision-making (business justification)\n",
        "\n",
        "Higher predictive performance: Ensembles (especially boosting) typically yield better accuracy, leading to better separation between defaulters and non-defaulters — reduces financial loss.\n",
        "\n",
        "Robustness: Ensembles reduce variance and are less sensitive to noisy features or outliers.\n",
        "\n",
        "Better risk stratification: More accurate probability estimates or rank-ordering allow for improved decision thresholds, targeted interventions (e.g., proactive collections), and better capital allocation.\n",
        "\n",
        "Feature interactions: Boosted trees automatically capture complex non-linear interactions present in transaction histories and demographics.\n",
        "\n",
        "Quantified uncertainty: Ensembles can provide more stable probability estimates with calibration, allowing monetary-risk calculations rather than binary decisions.\n",
        "\n",
        "Operational value: Fewer false negatives → fewer unexpected losses; fewer false positives → better customer experience and less lost revenue.\n",
        "\n",
        "12) Final checks & governance\n",
        "\n",
        "Validate on hold-out / temporal and external datasets if possible.\n",
        "\n",
        "Engage stakeholders (risk managers, compliance, product owners) to align thresholds and acceptable trade-offs.\n",
        "\n",
        "Maintain retraining schedule, rollback plan, and documentation for regulatory audits."
      ],
      "metadata": {
        "id": "CzuuiYBHX3YA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_izMpqMhXx_f"
      },
      "outputs": [],
      "source": []
    }
  ]
}